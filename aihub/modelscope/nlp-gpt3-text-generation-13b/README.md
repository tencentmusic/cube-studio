
# GPT3中文13B参数量文本生成模型
GPT-3模型是一个通用的预训练生成模型，使用Transformer的Decoder-only结构，可以用于解决下游各种类型的生成任务，特别是zero-shot生成能力。模型利用大量无监督数据，通过自回归任务进行预训练。可以用于解决文本生成相关的任务包含：文本摘要、问题生成、data-to-text等。

**Demo体验，请点击右侧进入AI写手创空间!!!**
## 模型描述
GPT-3模型使用Transformer的 Decoder结构，并对Transformer Decoder进行了一些改动，原本的Decoder包含了两个 Multi-Head Attention 结构，GPT-3只保留了 Mask Multi-Head Attention，利用常规的语言建模优化，从左到右的自回归预训练。本模型是基于GPT-3的代码结合大量中文无监督数据和下游任务数据预训练得到，我们训练了多种不同参数的模型，此处展示的是GPT-3 Base模型。GPT-3模型介绍，详见：[Language Models are Few-Shot Learners
](https://arxiv.org/abs/2005.14165)

本项目我们复现了一系列不同规模的中文GPT3模型，包括base/large/1.3B/2.7B/13B/30B/175B等，本模型是其中13B的版本。全部版本如下表所示：

|Model|Layers|Heads|d_model|LR|Batch|
|---|---|---|---|---|---|
|[base](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_chinese-base/summary)|12|12|768|6.0e-4|0.5M|
|[large](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_chinese-large/summary)|24|16|1024|3.0e-4|0.5M|
|[1.3B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_1.3B/summary)|24|32|2048|2.0e-4|2M|
|[2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B/summary)|32|32|2560|1.6e-4|2M|
|[13B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_13B/summary)|40|40|5120|1.0e-4|6M|
|[30B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_30B/summary)|48|56|7168|1.0e-4|6M|
|175B(work in process)|96|96|12288|1.2e-4|6M|

## 期望模型使用方式以及适用范围
本模型可直接用于文本生成，也可以通过finetune用于各类文本理解的任务。用户可以自行尝试各种输入文档。具体调用方式请参考代码示例。

### 如何使用
因模型规模太大，暂不支持下载，可以先使用右侧demo进行测试体验

### 模型局限性以及可能的偏差
模型训练数据来源于网络，生成结果可能存在一定偏差。

## 训练数据介绍
训练数据包括中文维基百科、网络上公开文本数据。

## 模型训练流程
本模型的训练分为两个阶段。第一阶段严格按照原始GPT3的参数设置进行：在中文wiki/ Common crawl等无监督数据上，通过自回归的训练任务训练了约300B字得到。第二阶段中，我们加入了多种有监督数据继续训练，使得模型具备多种任务的zero-shot的处理能力。

### 预处理
暂无

### 训练
暂无

### 推理加速
我们对大规模生成模型的推理速度进行了极致优化，13B模型128字的文本生成可以在1秒左右完成。

### 数据评估及结果
您可以在本页面右侧的在线测试中体验本模型的生成效果。输入测试文本，点击执行测试。其中，有两个可配置的参数：
- max_length: 最大长度设置，包含了输入和输出加起来的总最大长度
- top_k: sampling decode生成的配置参数，用于确定当前生成从词表概率分布的top_k进行采样，该值设置的越大，模型生成的多样性越丰富，但同时相关性和准确性会有所下降

模型可以应用于作文生成，代码生成，小说续写，诗词生成等，以下展示了几个模型结果的示例：
- 作文生成
```
输入：以《我的家乡杭州》为题写一篇作文

输出：我的家乡是杭州。“上有天堂,下有苏杭。”杭州因此享有“人间天堂”的美誉。杭州有很多有名的景点,如西湖、京杭大运河、灵隐寺、西溪湿地等。
提到西湖,你一定会想到杨贵妃和王羲之的名句“欲把西湖比西子,淡妆浓抹总相宜”。白娘子和许仙就是在西湖边相遇的,如今西湖边的景点已经成为爱情的象征。西湖,每年都有许许多多的人来这里游玩,当然,也有很多人对西湖这如诗如画的风景提出一些不足之处,比如西湖边的垃圾太多,水质变差了等。
如果你问我更喜欢杭州哪里,我会毫不犹豫地回答:“我更喜欢杭州的西湖。”要想欣赏西湖的美景,在白天最好是选择在曲院风荷或是苏堤上,一边呼吸着带有荷花香味的新鲜空气,一边欣赏着西湖的美景,让人心旷神怡。到了晚上,就可以到白堤、苏堤、杨公堤乘坐游船了,随着游船的不断前行,西湖的各个景点你都可以尽收眼底,原本由石头堆积而成的西湖,如今变成了由水泥和石头堆积而成的水上城市,让人赏心悦目。
西湖不仅景色美,而且还有许多好吃的小吃和土特产,其中西湖藕粉是我最爱吃的。早晨我喝上一杯西湖藕粉,不仅能开胃,还能养生。如果你来到我的家乡杭州,我一定会带你去品尝哦!

```

- 代码生成
```
输入：写一个python的快排代码

输出：
python3
class Solution:
 def quicksort(self, nums: List[int], l: List[int], r: List[int]) -> List[int]:
  if r == l:
   return nums
  def quick_sort(l, r):
   i = l-1
   j = r+1
   pivot = nums[l]
   while i < j:
    while nums[j] < pivot and i < j:
     j -= 1
    while nums[i] >= pivot and i < j:
     i += 1
    nums[i], nums[j] = nums[j], nums[i]
   nums[i], nums[r] = nums[r], nums[i]
   if i > r:
    quick_sort(l, i-1)
   elif i < r:
    quick_sort(i+1, r)
  quick_sort(0, len(nums)-1)
  return nums
```

- 小说续写
```
输入：话说宝玉和凤姐回家，见过众人，宝玉便回明贾母要约秦钟上家塾之事，自己也有个伴读的朋友，正好发愤；又着实称赞秦钟人品行事，最是可人怜爱的。凤姐又在一旁帮着说：“改日秦钟还来拜见老祖宗呢。”说的贾母喜欢起来。凤姐又趁势请贾母一同过去看戏。贾母虽年高

输出：话说宝玉和凤姐回家,见过众人,宝玉便回明贾母要约秦钟上家塾之事,自己也有个伴读的朋友,正好发愤;又着实称赞秦钟人品行事,最是可人怜爱的。凤姐又在一旁帮着说:“改日秦钟还来拜见老祖宗呢。”说的贾母喜欢起来。凤姐又趁势请贾母一同过去看戏。贾母虽年高,却也是个爱戏之人,当下随了宝玉、秦钟,来到戏房。只见贾瑞、贾蔷二人在那边看戏,秦钟在那边听秦钟和贾蔷在那里争执。秦钟和贾蔷又都是年轻人,又不相识。只见贾蔷因笑道:“好哥哥,你也来逛逛,这会子我还听戏哩!”说话之间,已来至戏房。
宝玉正在戏台上看《牡丹亭》,见贾瑞过来,便回过头来,向贾瑞一笑,又向凤姐儿一望,贾瑞便转身走了。凤姐见了他这般光景,也就随即回过头来。贾母笑道:“你又往那去了?作什么来你了?谁让你来的?”贾瑞道:“我往马圈去了。”凤姐道:“你快回去罢,看人撞见怎么样!”贾母道:“那里是撞见了?”贾瑞道:“我也不知是怎么了,心里就象有一件大事,只是还没放完,怕放完了,这会子才完。”凤姐笑道:“我不信,他心里大事一定是有人勾引了他。”贾母笑道:“那里这般事!你不知道你珍大哥哥那边的焦大,也是头一个撞着这么干净的人。”贾瑞笑道:“这会子他干净也是应当的,只是如今他又不知干净到那里去了。”凤姐笑道:“那么着,我就先去瞧瞧他。”贾母道:“不许去!等着人家去告诉他。”贾瑞道:“那么着,我先去告诉了你珍大哥哥,你就来。”凤姐笑道:“我不信。”贾母笑道:“不许去!我知道你珍大哥哥那边,不过我已去得久了,你自然也知道。”
```

- 诗词生成
```
输入：寒随穷律变，春逐鸟声开。初风飘带柳，

输出：寒随穷律变,春逐鸟声开。初风飘带柳,细雨润花梅。自喜幽栖好,从教俗虑催。
```

## 相关引用
我们将尽快推出本项目的技术报告，敬请期待！
