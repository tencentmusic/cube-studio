模型名,中文名,描述
'nlp-raner-named-entity-recognition-chinese-base-news','RaNER命名实体识别-中文-新闻领域-base','该模型是基于检索增强(RaNer)方法在中文MSRA数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-csanmt-translation-en2zh','CSANMT连续语义增强机器翻译-英中-通用领域-large','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-csanmt-translation-zh2en','CSANMT连续语义增强机器翻译-中英-通用领域-large','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-structbert-zero-shot-classification-chinese-base','StructBERT零样本分类-中文-base','该模型使用StructBERT-base在xnli_zh数据集(将英文数据集重新翻译得到中文数据集)上面进行了训练得到。'
'nlp-structbert-sentiment-classification-chinese-base','StructBERT情感分类-中文-通用-base','StructBERT情感分类-中文-通用-base是基于bdci、dianping、jd binary、waimai-10k四个数据集（11.5w条数据）训练出来的情感分类模型。'
'nlp-structbert-word-segmentation-chinese-base','BAStructBERT分词-中文-新闻领域-base','基于预训练语言模型的新闻领域中文分词模型，根据用户输入的中文句子产出分词结果。'
'nlp-raner-named-entity-recognition-chinese-base-ecom-50cls','RaNER命名实体识别-中文-电商领域-细粒度-base','该模型是基于检索增强(RaNer)方法在中文细粒度电商数据集训练的模型。本方法采用Transformer-CRF模型，使用sbert-base作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'promptclue','全中文任务支持零样本学习模型','支持近20中文任务，并具有零样本学习能力。 针对理解类任务，如分类、情感分析、抽取等，可以自定义标签体系；针对生成任务，可以进行采样自由生成。使用1000亿中文token（字词级别）进行大规模预训练，累计学习1.5万亿中文token，并且在100+任务上进行多任务学习获得。'
'nlp-raner-named-entity-recognition-chinese-base-cmeee','RaNER命名实体识别-中文-医疗领域-base','该模型是基于检索增强(RaNer)方法在中文CMeEE数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-convai-text2sql-pretrain-cn','SPACE-T表格问答预训练模型-中文-通用领域-base','SPACE-T表格问答预训练模型-中文-通用领域-base大规模预训练模型，基于transformers架构，在千万级中文表格，亿级中文表格训练数据上进行预训练，在中文跨领域、多轮、Text-to-SQL语义解析等任务上能取得很好的效果。'
'chatyuan-large','元语功能型对话大模型','元语功能型对话大模型这个模型可以用于问答、结合上下文做对话、做各种生成任务，包括创意性写作，也能回答一些像法律、新冠等领域问题。它基于PromptCLUE-large结合数亿条功能对话多轮对话数据进一步训练得到。'
'nlp-gpt3-text-generation-13b','GPT-3预训练生成模型-中文-13B','13B参数量的中文GPT-3文本生成模型'
'nlp-gpt3-text-generation-chinese-base','GPT-3预训练生成模型-中文-base','1亿参数量的中文GPT-3文本生成模型'
'nlp-gpt3-poetry-generation-chinese-large','GPT-3诗词生成模型-中文-large','3亿参数量的中文GPT-3诗词生成模型'
'promptclue-base-v1-5','全中文任务支持零样本学习模型v1.5','支持近20中文任务，并具有零样本学习能力。 针对理解类任务，如分类、情感分析、抽取等，可以自定义标签体系；针对生成任务，可以进行采样自由生成。使用1000亿中文token（字词级别）进行大规模预训练，累计学习1.5万亿中文token，并且在100+任务上进行多任务学习获得。'
'nlp-structbert-word-segmentation-chinese-base-ecommerce','BAStructBERT分词-中文-电商领域-base','基于预训练语言模型的电商领域中文分词模型，根据用户输入的中文句子产出分词结果。'
'nlp-bart-text-error-correction-chinese','BART文本纠错-中文-通用领域-large','我们采用seq2seq方法建模文本纠错任务。模型训练上，我们使用中文BART作为预训练模型，然后在Lang8和HSK训练数据上进行finetune。不引入额外资源的情况下，本模型在NLPCC18测试集上达到了SOTA。'
'nlp-rom-passage-ranking-chinese-base','ROM语义相关性-中文-通用领域-base','基于ROM-Base预训练模型的通用领域中文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-structbert-word-segmentation-chinese-lite','BAStructBERT分词-中文-新闻领域-lite','基于预训练语言模型的新闻领域中文分词模型，根据用户输入的中文句子产出分词结果。'
'nlp-mt5-zero-shot-augment-chinese-base','全任务零样本学习-mT5分类增强版-中文-base','该模型在mt5模型基础上使用了大量中文数据进行训练，并引入了零样本分类增强的技术，使模型输出稳定性大幅提升。支持任务包含：分类、摘要、翻译、阅读理解、问题生成等等。'
'nlp-structbert-word-segmentation-chinese-lite-ecommerce','BAStructBERT分词-中文-电商领域-lite','基于预训练语言模型的电商领域中文分词模型，根据用户输入的中文句子产出分词结果。'
'nlp-corom-sentence-embedding-chinese-base','CoROM文本向量-中文-通用领域-base','基于CoROM-base预训练语言模型的通用领域中文文本表示模型，基于输入的句子产出对应的文本向量，文本向量可以使用在下游的文本检索、句子相似度计算、文本聚类等任务中。'
'nlp-corom-sentence-embedding-chinese-base-ecom','CoROM文本向量-中文-电商领域-base','基于CoROM-base预训练语言模型的电商领域中文文本表示模型，基于输入的句子产出对应的文本向量，文本向量可以使用在下游的文本检索、句子相似度计算、文本聚类等任务中。'
'nlp-csanmt-translation-en2zh-base','CSANMT连续语义增强机器翻译-英中-通用领域-base','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-corom-passage-ranking-chinese-base-ecom','CoROM语义相关性-中文-电商领域-base','基于ROM-Base预训练模型的电商领域中文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-raner-named-entity-recognition-chinese-base-ecom','RaNER命名实体识别-中文-电商领域-base','该模型是基于检索增强(RaNer)方法在中文电商数据集训练的模型。本方法采用Transformer-CRF模型，使用sbert-base作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-structbert-sentiment-classification-chinese-ecommerce-base','StructBERT情感分类-中文-电商-base','StructBERT中文情感分类模型是基于百万电商评价数据训练出来的情感分类模型'
'nlp-plug-text-generation-27b','PLUG预训练生成模型-中文-27B','PLUG是一个270亿参数的大规模中文理解和生成联合预训练模型，由海量高质量中文文本预训练得到，在中文的多个下游理解和生成任务上，该模型效果达到state-of-the-art水平，且具有零样本生成能力。'
'nlp-lstmcrf-word-segmentation-chinese-news','LSTM分词-中文-新闻领域','char-BiLSTM-CRF中文新闻领域分词模型'
'punc-ct-transformer-zh-cn-common-vocab272727-pytorch','CT-Transformer标点-中文-通用-pytorch','中文标点通用模型：可用于语音识别模型输出文本的标点预测。'
'nlp-structbert-sentence-similarity-chinese-base','StructBERT文本相似度-中文-通用-base','StructBERT文本相似度-中文-通用-base是在structbert-base-chinese预训练模型的基础上，用atec、bq_corpus、chineseSTS、lcqmc、paws-x-zh五个数据集（52.5w条数据，正负比例0.48:0.52）训练出来的相似度匹配模型。由于license权限问题，目前只上传了BQ_Corpus、chineseSTS、LCQMC这三个数据集。'
'nlp-csanmt-translation-en2fr','CSANMT连续语义增强机器翻译-英法-通用领域-base','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-lstmcrf-word-segmentation-chinese-ecommerce','LSTM分词-中文-电商领域','char-biLSTM-CRF中文电商领域分词模型'
'nlp-corom-passage-ranking-english-base','CoROM语义相关性-英文-通用领域-base','基于CoROM-Base预训练模型的通用领域英文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-structbert-sentence-similarity-chinese-large','StructBERT文本相似度-中文-通用-large','StructBERT文本相似度-中文-通用-large是在structbert-large-chinese预训练模型的基础上，用atec、bq_corpus、chineseSTS、lcqmc、paws-x-zh五个数据集（52.5w条数据，正负比例0.48:0.52）训练出来的相似度匹配模型。'
'nlp-gpt3-kuakua-robot-chinese-large','GPT-3夸夸机器人-中文-large','GPT-3夸夸机器人，主要用于夸夸场景，我们训练的机器人可以针对用户的不同输入进行全方位无死角的夸，同时针对相同的输入重复调用模型会得到不同的夸奖词'
'u2pp-conformer-asr-cn-16k-online','WeNet-U2pp_Conformer-语音识别-中文-16k-实时','WeNet 是一款面向工业落地应用的语音识别工具包，提供了从语音识别模型的训练到部署的一条龙服务。我们使用 conformer 网络结构和 CTC/attention loss 联合优化方法，统一的流式/非流式语音识别方案，具有业界一流的识别效果；提供云上和端上直接部署的方案，最小化模型训练和产品落地之间的工程工作；框架简洁，模型训练部分完全基于 pytorch 生态，不依赖于 kaldi 等复杂的工具。 详细的注释和文档，非常适合用于学习端到端语音识别的基础知识和实现细节。 支持时间戳，对齐，端点检测，语言模型等相关功能。'
'nlp-bert-document-segmentation-chinese-base','BERT文本分割-中文-通用领域','该模型基于wiki-zh公开语料训练，对未分割的长文本进行段落分割。提升未分割文本的可读性以及下游NLP任务的性能。'
'nlp-raner-named-entity-recognition-chinese-base-generic','RaNER命名实体识别-中文-通用领域-base','该模型是基于检索增强(RaNer)方法在中文Ontonotes4.0数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-corom-sentence-embedding-english-base','CoROM文本向量-英文-通用领域-base','基于CoROM-Base预训练语言模型的通用领域英文文本表示模型，基于输入的句子产出对应的连续文本向量，改文本向量可以使用在下游的文本检索、句子相似度计算、文本聚类等任务中。'
'erlangshen-roberta-330m-sentiment','二郎神-RoBERTa-330M-情感分类','二郎神-RoBERTa-330M-情感分类'
'nlp-corom-sentence-embedding-chinese-base-medical','CoROM文本向量-中文-医疗领域-base','基于ROM-Base预训练模型的医疗领域中文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-gpt3-text-generation-chinese-large','GPT-3预训练生成模型-中文-large','3亿参数量的中文GPT-3文本生成模型'
'nlp-structbert-sentiment-classification-chinese-large','StructBERT情感分类-中文-通用-large','StructBERT情感分类-中文-通用-large是基于bdci、dianping、jd binary、waimai-10k四个数据集（11.5w条数据）训练出来的情感分类模型'
'nlp-csanmt-translation-en2es','CSANMT连续语义增强机器翻译-英西-通用领域-base','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-raner-named-entity-recognition-chinese-large-generic','RaNER命名实体识别-中文-通用领域-large','该模型是基于检索增强(RaNer)方法在中文数据集MultiCoNER-ZH-Chinese训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-ecom','RaNER命名实体识别-英语-电商领域-large','该模型是基于检索增强(RaNer)方法在英语电商query和商品标题数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-chinese-base-book','RaNER命名实体识别-中文-小说领域-base','该模型是基于检索增强(RaNer)方法在中文Book9小说领域数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-structbert-outbound-intention-chinese-tiny','StructBERT意图识别-中文-外呼-tiny','本模型基于StructBERT-tiny模型，使用外呼场景下的对话意图识别数据进行微调得到的。'
'nlp-lstm-named-entity-recognition-chinese-generic','LSTM命名实体识别-中文-通用领域','本方法采用char-BiLSTM-CRF模型'
'nlp-structbert-faq-question-answering-chinese-base','StructBERT FAQ问答-中文-通用领域-base','FAQ问答模型以StructBERT预训练模型-中文-base为基础，使用简单的原型网络，通过小样本meta-learning的方式在海量业务数据预训练(亿级)、微调(百万级)，在多个公开数据上取得了非常好的效果，适用于FAQ问答任务和小样本分类任务；'
'nlp-structbert-abuse-detect-chinese-tiny','StructBERT辱骂风险识别-中文-外呼-tiny','本模型基于StructBERT-tiny模型，使用外呼场景下的辱骂风险识别数据集训练得到。'
'nlp-corom-passage-ranking-chinese-base-medical','CoROM语义相关性-中文-医疗领域-base','基于CoROM-Base预训练模型的医疗领域中文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-bert-sentiment-analysis-english-base','BERT情感分析-英文-base-TweetEval数据集','该模型基于bert-base-uncased，在TweetEval数据集上fine-tune得到'
'nlp-raner-named-entity-recognition-chinese-base-resume','RaNER命名实体识别-中文-简历领域-base','该模型是基于检索增强(RaNer)方法在中文Resume数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'erlangshen-roberta-110m-sentiment','二郎神-RoBERTa-110M-情感分类',''
'nlp-lstm-named-entity-recognition-chinese-resume','LSTM命名实体识别-中文-简历领域','本方法采用char-BiLSTM-CRF模型'
'nlp-raner-named-entity-recognition-english-large-generic','RaNER命名实体识别-英语-通用领域-large','该模型是基于检索增强(RaNer)方法在MultiCoNER领域数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-chinese-base-social-media','RaNER命名实体识别-中文-社交媒体领域-base','该模型是基于检索增强(RaNer)方法在中文Weibo数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-multilingual-large-generic','RaNER命名实体识别-多语言统一-通用领域-large','该模型是基于检索增强(RaNer)方法在多语言数据集MultiCoNER-MULTI-Multilingual训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-mt5-dialogue-rewriting-chinese-base','MT5开放域多轮对话改写-中文-通用-base','开放域多轮对话改写模型主要解决开放域对话中的指代和省略问题，输入对话上下文，输出改写后的语义完整的问题。该模型基于google/mt5-base基座在公开数据和业务数据集上finetune而得，适用于开放域对话场景。'
'nlp-raner-named-entity-recognition-chinese-base-game','RaNER命名实体识别-中文-游戏领域-base','该模型是基于检索增强(RaNer)方法在中文游戏领域数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-structbert-part-of-speech-chinese-base','BAStructBERT词性标注-中文-新闻领域-base','基于预训练语言模型的新闻领域中文词性标注模型，根据用户输入的中文句子产出词性标注结果。'
'erlangshen-roberta-110m-similarity','二郎神-RoBERTa-110M-文本相似度',''
'nlp-raner-named-entity-recognition-chinese-base-literature','RaNER命名实体识别-中文-文学领域-base','该模型是基于检索增强(RaNer)方法在中文Literature数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-news','RaNER命名实体识别-英文-新闻领域-large','该模型是基于检索增强(RaNer)方法在英文conll03/conllpp领域数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-lstm-named-entity-recognition-chinese-news','LSTM命名实体识别-中文-新闻领域','本方法采用char-BiLSTM-CRF模型'
'nlp-raner-named-entity-recognition-chinese-base-bank','RaNER命名实体识别-中文-银行领域-base','该模型是基于检索增强(RaNer)方法在中文Bank数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-chinese-base-finance','RaNER命名实体识别-中文-金融领域-base','该模型是基于检索增强(RaNer)方法在CCKS2021中文金融案件要素抽取数据训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-csanmt-translation-en2ru-base','CSANMT连续语义增强机器翻译-英俄-通用领域-base','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-structbert-sentiment-classification-chinese-tiny','StructBERT情感分类-中文-通用-tiny','StructBERT情感分类-中文-通用-tiny是基于bdci、dianping、jd binary、waimai-10k四个数据集（11.5w条数据）训练出来的情感分类模型。'
'nlp-structbert-part-of-speech-chinese-lite','BAStructBERT词性标注-中文-新闻领域-lite','基于预训练语言模型的新闻领域中文词性标注模型，根据用户输入的中文句子产出词性标注结果。'
'nlp-raner-named-entity-recognition-turkish-large-generic','RaNER命名实体识别-土耳其语-通用领域-large','该模型是基于检索增强(RaNer)方法在土耳其语数据集MultiCoNER-TR-Turkish训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-korean-large-generic','RaNER命名实体识别-韩语-通用领域-large','该模型是基于检索增强(RaNer)方法在韩语数据集MultiCoNER-KO-Korean训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-lstm-named-entity-recognition-chinese-social-media','LSTM命名实体识别-中文-社交媒体领域','本方法采用char-BiLSTM-CRF模型。'
'nlp-raner-named-entity-recognition-dutch-large-generic','RaNER命名实体识别-荷兰语-通用领域-large','该模型是基于检索增强(RaNer)方法在荷兰语数据集MultiCoNER-NL-Dutch训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-russian-large-generic','RaNER命名实体识别-俄语-通用领域-large','该模型是基于检索增强(RaNer)方法在俄语数据集MultiCoNER-RU-Russian训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-ai','RaNER命名实体识别-英文-人工智能领域-large','该模型是基于检索增强(RaNer)方法在英文AI数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-hindi-large-generic','RaNER命名实体识别-印地语-通用领域-large','该模型是基于检索增强(RaNer)方法在MultiCoNER领域数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-music','RaNER命名实体识别-英文-音乐领域-large','该模型是基于检索增强(RaNer)方法在英文Music数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-lstmcrf-part-of-speech-chinese-news','LSTM词性标注-中文-新闻领域',''
'nlp-raner-named-entity-recognition-farsi-large-generic','RaNER命名实体识别-波斯语-通用领域-large','该模型是基于检索增强(RaNer)方法在波斯语数据集MultiCoNER-FA-Farsi训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-csanmt-translation-fr2en','CSANMT连续语义增强机器翻译-法英-通用领域-base','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-raner-named-entity-recognition-german-large-generic','RaNER命名实体识别-德语-通用领域-large','该模型是基于检索增强(RaNer)方法在德语数据集MultiCoNER-DE-German训练的模型。本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-literature','RaNER命名实体识别-英文-文学领域-large','该模型是基于检索增强(RaNer)方法在英文Literature数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-structbert-sentence-similarity-chinese-tiny','StructBERT文本相似度-中文-通用-tiny','StructBERT文本相似度-中文-通用-tiny是在structbert-tiny-chinese预训练模型的基础上，用atec、bq_corpus、chineseSTS、lcqmc、paws-x-zh五个数据集（52.5w条数据，正负比例0.48:0.52）训练出来的相似度匹配模型'
'nlp-raner-named-entity-recognition-english-large-wiki','RaNER命名实体识别-英语-wiki领域-large','本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-social-media','RaNER命名实体识别-英文-社交媒体领域-large','该模型是基于检索增强(RaNer)方法在英文wnut17领域数据集训练的模型。本方法采用Transformer-CRF模型，使用StructBERT作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-spanish-large-generic','RaNER命名实体识别-西班牙语-通用领域-large','该模型是基于检索增强(RaNer)方法在西班牙语数据集MultiCoNER-ES-Spanish训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-bangla-large-generic','RaNER命名实体识别-孟加拉语-通用领域-large','该模型是基于检索增强(RaNer)方法在孟加拉语数据集MultiCoNER-BN-Bangla训练的模型。 本方法采用Transformer-CRF模型，使用XLM-RoBERTa作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-ponet-fill-mask-chinese-base','PoNet预训练模型-中文-base','nlp_ponet_fill-mask_chinese-base是用中文wiki训练的预训练PoNet模型。'
'nlp-raner-named-entity-recognition-russian-large-ecom','RaNER命名实体识别-俄语-电商领域-large','该模型是基于检索增强(RaNer)方法在俄语电商query数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-politics','RaNER命名实体识别-英文-政治领域-large','该模型是基于检索增强(RaNer)方法在英文Politics数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-spanish-large-ecom','RaNER命名实体识别-西班牙语-电商领域-large','该模型是基于检索增强(RaNer)方法在西班牙语电商query数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-raner-named-entity-recognition-english-large-science','RaNER命名实体识别-英文-科学领域-large','该模型是基于检索增强(RaNer)方法在英文Science数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-masts-sentence-similarity-clue-chinese-large','MaSTS文本相似度-中文-搜索-CLUE语义匹配-large','MaSTS中文文本相似度-CLUE语义匹配模型是在MaSTS预训练模型-CLUE语义匹配的基础上，在QBQTC数据集上训练出来的相似度匹配模型。在CLUE语义匹配榜上通过集成此模型获得了第一名的成绩。'
'nlp-raner-named-entity-recognition-french-large-ecom','RaNER命名实体识别-法语-电商领域-large','该模型是基于检索增强(RaNer)方法在法语电商query数据集训练的模型。本方法采用Transformer-CRF模型，使用xlm-roberta-large作为预训练模型底座，结合使用外部工具召回的相关句子作为额外上下文，使用Multi-view Training方式进行训练。'
'nlp-csanmt-translation-es2en','CSANMT连续语义增强机器翻译-西英-通用领域-base','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'nlp-structbert-zero-shot-classification-chinese-large','StructBERT零样本分类-中文-large','该模型使用StructBERT-large在xnli_zh数据集(将英文数据集重新翻译得到中文数据集)上面进行了训练得到。'
'multilingual-glm-summarization-zh','mGLM多语言大模型-生成式摘要-中文','mGLM多语言大模型可从大段文本中提取关键信息，为你生成简短的中文摘要，支持多种语言输入'
'nlp-bart-text-error-correction-chinese-law','BART文本纠错-中文-法律领域-large','法研杯2022文书校对赛道冠军纠错模型（单模型）。'
'nlp-structbert-nli-chinese-base','StructBERT自然语言推理-中文-通用-base','StructBERT自然语言推理-中文-通用-base是在structbert-base-chinese预训练模型的基础上，用CMNLI、OCNLI两个数据集（45.8w条数据）训练出来的自然语言推理模型。'
'nlp-bert-relation-extraction-chinese-base','RoBERTa关系抽取-中文-通用-base','百科关系抽取模型是在hfl/chinese-roberta-wwm-ext预训练模型的基础上，用duie数据集训练出来的关系抽取模型。'
'nlp-structbert-fact-checking-chinese-base','StructBERT事实准确性检测-中文-电商-base','StructBERT事实准确性检测-中文-电商-base是在structbert-base-chinese预训练模型的基础上，使用业务数据训练出的自然语言推理模型，用于事实准确性检测，输入两个句子，判断两个句子描述的事实是否一致。'
'nlp-structbert-sentence-similarity-chinese-retail-base','StructBERT文本相似度-中文-电商-base','StructBERT中文电商域文本相似度模型是在structbert-base-chinese预训练模型的基础上，用电商域标注数据训练出来的相似度匹配模型。'
'nlp-bert-document-segmentation-english-base','BERT文本分割-英文-通用领域','该模型基于wiki-en公开语料训练，对未分割的长文本进行段落分割。提升未分割文本的可读性以及下游NLP任务的性能。'
'nlp-structbert-zero-shot-classification-chinese-tiny','StructBERT零样本分类-中文-tiny','该模型使用StructBERT-base在xnli_zh数据集(将英文数据集重新翻译得到中文数据集)上面进行了训练得到。'
'nlp-structbert-nli-chinese-large','StructBERT自然语言推理-中文-通用-large','StructBERT自然语言推理-中文-通用-large是在structbert-large-chinese预训练模型的基础上，用CMNLI、OCNLI两个数据集（45.8w条数据）训练出来的自然语言推理模型。'
'nlp-gpt3-text-generation-30b','GPT-3预训练生成模型-中文-30B',''
'nlp-star-conversational-text-to-sql','SPACE-T表格问答预训练模型-英文-通用领域-Large','本项目是多轮Text-to-SQL模型，可针对不同领域数据库和用户直接进行多轮对话，生成相应的SQL查询语句。用户可以在对话过程中表达自己对数据库模式的查询要求，并在系统的帮助下生成符合要求的SQL查询语句。'
'nlp-ponet-extractive-summarization-topic-level-chinese-base','PoNet抽取式话题摘要模型-中文-base-ICASSP2023-MUG-Track2','该模型基于PoNet模型架构，在AliMeeting4MUG Corpus训练，进行抽取式话题摘要任务。'
'guohua-diffusion','国画Diffusion模型','这是在国画上训练的微调Stable Diffusion模型'
'nlp-xlmr-named-entity-recognition-indo-ecommerce-title','XLM-R命名实体识别-印尼语-电商领域(商品标题)-base','XLM-R命名实体识别-印尼语-电商领域(商品标题)-base是基于20K电商领域商品标题数据训练得到的印尼语命名实体识别模型，可根据用户输入的印尼语商品标题文本产出命名实体识别结果。'
'nlp-structbert-nli-chinese-tiny','StructBERT自然语言推理-中文-通用-tiny','StructBERT自然语言推理-中文-通用-tiny是在structbert-tiny-chinese预训练模型的基础上，用CMNLI、OCNLI两个数据集（45.8w条数据）训练出来的自然语言推理模型'
'nlp-bert-sentence-similarity-english-base','BERT文本相似度-英文-base-学术数据集paws','该模型是在bert-base-uncased预训练模型的基础上，用paws数据集训练出来的文本相似度匹配模型。'
'nlp-domain-classification-chinese','FastText文本领域分类-中文-国民经济行业18大类','用于中文的文本领域分类，分类依据为国民经济行业分类（GB/T 4754—2017），原分类标准有20大类，目前支持18个行业的分类：交通运输仓储邮政\住宿餐饮\信息软件\农业\制造业\卫生医疗\国际组织\建筑\房地产\政府组织\教育\文体娱乐\水利环境\电力燃气水生产\科学技术\租赁法律\采矿\金融。'
'nlp-structbert-faq-question-answering-chinese-finance-base','StructBERT FAQ问答-中文-金融领域-base','金融领域FAQ问答模型以StructBERT FAQ问答-中文-通用领域-base模型为基础，在金融领域数据上微调得到，适用于金融领域FAQ问答任务，包括但不局限于：银行、保险等场景；'
'nlp-xlmr-word-segmentation-thai','XLM-R分词-泰语-通用领域-base','XLM-R分词-泰语-通用领域-base是基于BEST-2010数据训练得到的泰语分词模型，可根据用户输入的泰语文本产出分词结果。'
'nlp-corom-passage-ranking-chinese-tiny','CoROM语义相关性-中文-通用领域-tiny','基于ROM-tiny预训练模型的通用领域中文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-corom-sentence-embedding-chinese-tiny-medical','CoROM文本向量-中文-医疗领域-tiny','基于CoROM-tiny预训练语言模型的电商领域中文文本表示模型，基于输入的句子产出对应的文本向量，文本向量可以使用在下游的文本检索、句子相似度计算、文本聚类等任务中。'
'nlp-bert-zero-shot-english-base','BERT零样本分类-英文-base-学术数据集mnli','该模型使用bert-base-uncased在multi_nli数据集(将英文数据集重新翻译得到中文数据集)上面进行了训练得到。'
'nlp-structbert-keyphrase-extraction-base-icassp2023-mug-track4-baseline','StructBert关键词抽取-中文-base-ICASSP2023-MUG-Track4','ICASSP2023 MUG Track4 关键词抽取Baseline'
'nlp-corom-sentence-embedding-chinese-tiny','CoROM文本向量-中文-通用领域-tiny','基于CoROM-base预训练语言模型的通用领域中文文本表示模型，基于输入的句子产出对应的文本向量，文本向量可以使用在下游的文本检索、句子相似度计算、文本聚类等任务中。'
'nlp-corom-sentence-embedding-chinese-tiny-ecom','CoROM文本向量-中文-电商领域-tiny','基于CoROM-tiny预训练语言模型的电商领域中文文本表示模型，基于输入的句子产出对应的文本向量，文本向量可以使用在下游的文本检索、句子相似度计算、文本聚类等任务中。'
'nlp-corom-sentence-embedding-english-tiny','CoROM文本向量-英文-通用领域-tiny','基于CoROM-Base预训练模型的通用领域英文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-structbert-faq-question-answering-chinese-gov-base','StructBERT FAQ问答-中文-政务领域-base','政务领域FAQ问答模型以StructBERT FAQ问答-中文-通用领域-base模型为基础，在政务领域数据上微调得到，适用于政务领域FAQ问答任务，包括但不局限于社保、公积金等场景；'
'nlp-bert-entity-embedding-chinese-base','Bert实体向量-中文-通用领域-base',''
'nlp-user-satisfaction-estimation-chinese','HiTransUSE用户满意度估计-中文-电商-base','支持对话级的用户满意度分析，输出（不满意，中立，满意）三种标签'
'nlp-ponet-extractive-summarization-doc-level-chinese-base','PoNet抽取式篇章摘要模型-中文-base-ICASSP2023-MUG-Track2','该模型基于PoNet模型架构，在AliMeeting4MUG Corpus训练，进行抽取式篇章摘要任务。'
'nlp-bert-entity-matching-chinese-base','Bert实体相关性-中文-通用领域-base','输入带实体标记的句子A，以及一个候选句子列表，模型输出句子A中的实体与列表中每个候选句子的相关性得分（0-1，分数越高代表两者越相关），'
'nlp-nested-ner-named-entity-recognition-chinese-base-med','NestedNER命名实体识别-中文-医疗领域-base',''
'nlp-ponet-document-segmentation-topic-level-chinese-base','PoNet文本话题分割模型-中文-base-ICASSP2023-MUG-Track1','该模型基于PoNet模型架构，在AliMeeting4MUG Corpus训练，对带段落的长文本进行中文话题分割。'
'nlp-corom-passage-ranking-chinese-tiny-ecom','CoROM语义相关性-中文-电商领域-tiny','基于CoROM-Base预训练模型的电商领域中文语义相关性模型，模型以一个source sentence以及一个句子列表作为输入，最终输出source sentence与列表中每个句子的相关性得分（0-1，分数越高代表两者越相关）。'
'nlp-bert-relation-extraction-chinese-base-commerce','DIRECT商品评价解析-中文-电商-base',''
'nlp-structbert-siamese-uie-chinese-base','SiameseUIE通用信息抽取-中文-base',''
'nlp-xlmr-named-entity-recognition-viet-ecommerce-title','XLM-R命名实体识别-越南语-电商领域(商品标题)-base','XLM-R命名实体识别-越南语-电商领域(商品标题)-base是基于20K电商领域商品标题数据训练得到的越南语命名实体识别模型，可根据用户输入的越南语商品标题文本产出命名实体识别结果。'
'nlp-structbert-fill-mask-chinese-large','StructBERT完形填空模型-中文-large','nlp_structbert_fill-mask_chinese-large是海量中文数据训练的自然语言理解预训练模型。'
'nlp-csanmt-translation-ru2en-base','CSANMT连续语义增强机器翻译-俄英-通用领域-base','基于连续语义增强的神经机器翻译模型以有限的训练样本为锚点，学习连续语义分布以建模全局的句子空间，并据此构建神经机器翻译引擎，有效提升数据的利用效率，显著改善模型的泛化能力和鲁棒性。'
'uni-fold-monomer','Uni-Fold-Monomer 开源的蛋白质单体结构预测模型','开源的蛋白质单体结构预测模型，输入蛋白质单体的一级结构（1D序列），预测蛋白质的三级结构（3D位置），同时给出预测结果的置信度。本模型主要用于蛋白质单体的预测。'
'nlp-xlmr-named-entity-recognition-eng-ecommerce-title','XLM-R命名实体识别-英语-电商领域(商品标题)-base','XLM-R命名实体识别-英语-电商领域(商品标题)-base是基于20K电商领域商品标题数据训练得到的英语命名实体识别模型，可根据用户输入的英语商品标题文本产出命名实体识别结果。'
'nlp-veco-fill-mask-large','VECO完形填空模型-多语言-large','nlp_veco_fill-mask-large是CommonCrawl Corpus训练的自然语言理解多语言预训练模型。'
'nlp-bert-fill-mask-chinese-base','BERT完形填空模型-中文-base','nlp_bert_fill-mask_chinese-base 是wikipedia_zh/baike/news训练的自然语言理解预训练模型。'
'glm130b','GLM130B-中英大模型','模型集成中'
'nlp-mgimn-faq-question-answering-chinese-base','MGIMN FAQ问答-中文-通用领域-base','MGIMN FAQ问答模型以StructBERT预训练模型为底座，采用多维度交互式匹配模型网络结构，通过小样本meta-learning的方式在海量业务数据预训练(亿级)、微调(百万级)得到，相对于StructBERT模型效果更优，适用于FAQ问答任务和通用小样本分类任务；'
'nlp-unite-mup-translation-evaluation-multilingual-large','统一的翻译质量评价模型-多语言-新闻领域-large','Kendall's Tau on WMT'19 Metrics Shared Task dataset'
'nlp-space-dialog-state-tracking','SPACE对话状态追踪-英文-base','该模型是 SPACE 基于一个对话状态跟踪数据集 MultiWOZ2.2 微调后的下游模型，称作 space_dialog-state-tracking，可专门用来做旅游、餐馆等领域的对话状态跟踪任务。'
'nlp-mgimn-faq-question-answering-chinese-finance-base','MGIMN FAQ问答-中文-金融领域-base','金融领域MGIMN FAQ问答模型以MGIMN FAQ问答-中文-通用领域-base模型为基础，在金融领域数据上微调得到，适用于金融领域FAQ问答任务，包括但不局限于：银行、保险等场景；MGIMN系列模型相对于StructBERT FAQ系列模型效果更优；'
'nlp-structbert-emotion-classification-chinese-base','StructBERT情绪分类-中文-七分类-base',''
'nlp-unite-up-translation-evaluation-english-large','统一的翻译质量评价模型-英语-新闻领域-large','翻译质量评价，即对翻译文本进行质量评估，在给定源端输入、目标端参考答案、或两者均有提供的情况下，算法用于评估所生成文本的质量。本单一模型可同时支持提供源端输入（src-only）、目标端参考译文（ref-only）、或者两者均有（src-ref-combined）三种评价场景。 模型由一个预训练语言模型（Pretrained Language Model）和一个前馈神经网络（Feedforward Network）组成。模型首先在伪语料上进行继续预训练，而后在WMT'17-18 Metrics Shared Task数据集上进行微调。 此模型为目标语为英语的large版本。'
'nlp-translation-quality-estimation-multilingual','QEMind翻译质量评估-多语言-通用领域','对翻译质量进行打分评估的模型，支持多个语向，获WMT 2021世界机器翻译大赛质量评估DA子任务冠军'
'codegeex-code-translation-13b','CodeGeeX-代码翻译-13B','CodeGeeX是一个具有130亿参数的多编程语言代码生成预训练模型，在20多种编程语言的代码语料库（>8500亿Token）上经过历时两个月预训练得到。CodeGeeX采用华为MindSpore框架实现，在鹏城实验室的“鹏城云脑ll”平台上训练而成。'
'uni-fold-multimer','Uni-Fold-Multimer 开源的蛋白质复合物结构预测模型','一个开源的蛋白质复合物结构预测模型。'
'multilingual-glm-summarization-en','mGLM多语言大模型-生成式摘要-英文','mGLM多语言大模型可从大段文本中提取关键信息，为你生成简短的英文摘要，支持多种语言输入'
'mo-di-diffusion','迪士尼风格扩散生成模型','在迪士尼动画电影截图上finetune的Stable Diffusion模型'
'analog-diffusion','胶片质感扩散生成模型','输出胶片摄影质感的图像；微调自Stable Diffusion'
'nlp-ponet-fill-mask-english-base','PoNet预训练模型-英文-base','nlp_ponet_fill-mask_english-base是用bookcorpus/wikitext训练的预训练PoNet模型。'
'nlp-medical-structbert-backbone-base','StructBERT预训练模型-中文-医疗领域-base','医疗StructBERT预训练模型'
