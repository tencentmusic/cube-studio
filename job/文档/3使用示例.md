[[toc]]  
  
# 一.多优化器使用示例  
这里使用tensorflow2.3官方的Wide&deep模型keras实现作为例子，这个模型里面包含linear和dnn两个部分，每个部分使用不同的优化器进行训练。Wide&deep模型说明见https://www.tensorflow.org/api_docs/python/tf/keras/experimental/WideDeepModel  
接入平台的代码如下：  
  
```   
  
import numpy as np  
import tensorflow as tf  
from tensorflow.keras.experimental import WideDeepModel, LinearModel  
  
  
w = np.random.randn(32)  
b = np.random.randn()  
print("w={}\nb={}".format(w, b))  
  
# 模拟数据集生成器  
def data_gen(num_samples=2000000):  
    for i in range(num_samples):  
        x1 = np.random.randn(32),  
        x2 = np.random.randn(32),  
        y1 = tf.reduce_sum(w*x1) + b  
        y2 = tf.sin(tf.reduce_mean(x2))  
        yield (x1, x2), [y1+y2]  
  
  
def __create_dataset(num_samples, batch_size):  
    return tf.data.Dataset.from_generator(lambda: data_gen(num_samples), ((tf.float32, tf.float32), tf.float32),  
                                          (([1, 32], [1, 32]), [1])).batch(batch_size)  
  
# 创建训练集  
def awf_create_train_dataset_fn(num_samples, batch_size):  
    return __create_dataset(num_samples, batch_size)  
  
# 创建验证集（非必须）  
def awf_create_val_dataset_fn(num_samples, batch_size):  
    return __create_dataset(num_samples, batch_size)  
  
# 创建测试集（非必须）  
def awf_create_test_dataset_fn(num_samples, batch_size):  
    return __create_dataset(num_samples, batch_size)  
  
# 创建模型，这里直接使用了tf的官方实现  
def awf_create_model_fn():  
    linear_model = LinearModel()  
    dnn_model = tf.keras.Sequential([tf.keras.layers.Dense(units=16),  
                                     tf.keras.layers.Dense(units=1)])  
    return WideDeepModel(linear_model, dnn_model)  
  
# 参数分组（分成两组，每组对应不同优化器，这里tf官方实现上对linear和dnn分别使用了两个子模型封装，自己实现时不一定受这个限制，参数集能正确分组就好）  
def awf_group_trainable_vars_fn(model: WideDeepModel):  
    return model.linear_model.trainable_variables, model.dnn_model.trainable_variables  
  
```  
训练任务配置，使用两个优化器，linear部分使用SGD，dnn部分使用Adam，对应配置如下所示：  
  
```   
{  
    "num_workers": 2,  
    "node_affin": "pref_gpu",  
    "pod_affin": "spread",  
    "timeout": "100d",  
    "trainer": "runner",  
    "job_detail": {  
        "script_name": "wnd.py",  
        "model_args": {},  
        "train_data_args": {  
            "num_samples": 20000,  
            "shard_policy": "DATA"  
        },  
        "test_data_args": {  
            "num_samples": 5000,  
            "shard_policy": "DATA"  
        },  
        "train_args": {  
            "mw_com": "RING",  
            "verboase": true,  
            "train_type": "compile_fit",  
            "batch_size": 512,  
            "epochs": 3,  
            "num_samples": 20000,  
            "validation_steps": 10,  
            "optimizer": [  
                {  
                    "type": "sgd",  
                    "args": {  
                        "learning_rate": 0.01  
                    }  
                },  
                {  
                    "type": "adam",  
                    "args": {  
                        "learning_rate": 0.01  
                    }  
                }  
            ],  
            "losses": "mse",  
            "metrics": [  
                "mse"  
            ]  
        }  
    }  
```  
上面训练任务的每一个字段详细说明请参见http://tapd.oa.com/kubeflow/markdown_wikis/show/#1220424693001722117@toc5 ，这里需要特别说明的一点是mw_com配置，这个字段用于指定tf分布式训练的通信后端，经过测试和网上调研，!!#ff00ff 在使用多优化器和分布式GPU训练时，NCCL似乎有bug，只能采用RING方式，单优化器和单机以及分布式cpu都没有这个问题。!!  
# 二.多机训练示例  
这里使用mnist作为例子，只做训练，因此只实现了创建模型和训练数据两个回调函数。接入平台代码如下：  
  
```   
import tensorflow as tf  
import tensorflow_datasets as tfds  
from tensorflow.keras import layers, models  
'''  
多机训练和单机训练代码是完全一致，在代码层无感知，在任务配置中配置多个worker就行了  
'''  
# 创建模型  
def awf_create_model_fn(name):  
    model = models.Sequential(name=name)  
    model.add(  
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  
    model.add(layers.MaxPooling2D((2, 2)))  
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))  
    model.add(layers.MaxPooling2D((2, 2)))  
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))  
    model.add(layers.Flatten())  
    model.add(layers.Dense(64, activation='relu'))  
    model.add(layers.Dense(10, activation='softmax'))  
  
    model.summary()  
    return model  
  
# 创建训练数据集  
def awf_create_train_dataset_fn(data_dir, batch_size):  
  
    # Scaling MNIST data from (0, 255] to (0., 1.]  
    def scale(image, label):  
        image = tf.cast(image, tf.float32)  
        image /= 255  
        return image, label  
  
    ds, info = tfds.load(name='mnist', with_info=True, as_supervised=True, data_dir=data_dir, download=False)  
  
    return ds['train'].batch(batch_size).map(scale).cache().shuffle(10000)  
  
```  
训练任务配置如下，这里配置了4个worker，并且使用gpu（这个例子里面事先下载好了训练数据并放在了包目录的train_data目录下）：  
  
```   
{  
    "namespace": "pipeline",  
    "num_workers": 4,  
    "node_affin": "pref_gpu",  
    "pod_affin": "spread",  
    "timeout": "100d",  
    "trainer": "runner",  
    "job_detail": {  
        "script_name": "mnist.py",  
        "model_args": {  
            "name": "mnist_bench"  
        },  
        "train_data_args": {  
            "data_dir": "${PACK_PATH}$/train_data"  
        },  
        "train_args": {  
            "mw_com": "NCCL",  
            "verboase": true,  
            "train_type": "compile_fit",  
            "batch_size": 512,  
            "epochs": 5,  
            "num_samples": 6000000,  
            "optimizer": {  
                "type": "adam",  
                "args": {  
                    "learning_rate": 0.001  
                }  
            },  
            "losses": "scce",  
            "metrics": [  
                "scacc"  
            ]  
        }  
    }  
}  
```  
  
  
