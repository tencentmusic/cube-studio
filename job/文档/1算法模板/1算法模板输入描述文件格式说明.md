# 一. 整体结构  
“算法模板输入描述文件”用于描述模板模型实例化时的输入属性及需要做的处理，文件内容是json格式，总体格式如下所示  
  
```   
{  
	"inputs": [  
		{<input_desc>},  
		...  
	],  
	"feature_columns": {  
		"defs": [  
			{<fc_desc>},  
			...  
		]  
	}  
}  
```  
描述文件分为两个大的部分:  
1.**"inputs"**: 是一个数组，描述原始输入有哪些特征列以及所需要的转换（即其中的每一个input_desc），inputs字段是必填的。  
2.**"feature_columns"**：这个主要是为了支持在原始输入基础上利用feature_column做交叉。其中的"defs"字段是一个数组，描述使用feature column对原始输入做哪些交叉和转换（即其中的每一个fc_desc），feature_columns不是必填字段，如果没有特征交叉需求，一般可以不填这个字段。  
# 二.原始输入描述格式(input_desc)  
原始输入描述是一个json dict，其中包含的字段有如下一些：  
  
| 字段名 | 是否必填 | 取值类型 | 说明 |  
| ------ | ------ | ------ | ------ |  
| **name** | 是 | string | 特征名称 |  
| **dtype** | 是 | string | 特征的数据类型，支持的类型有：int8、int16、int32、int64、float16、float32、float64、string（!!#e06666  *注意：当val_sep不为空时，dtype应该填拆分后的单个元素类型* !!） |  
|**shape**|否|int/list|单个特征值的形状，不填的情况下，默认是(1, )|  
|**val_sep**|否|string|当特征是一个序列，并且序列用分隔符串联起来表示时，val_sep设置使用的分隔符。例如"123,234,456"，这样原始输入可以不需要做拆分直接输入即可。|  
|**default_value**|否|与dtype对应|特征默认值，当不填时，会根据特征的dtype推断。|  
|**vocab_size**|否|int|当是类别特征时，可以通过vocab_size设置分类数。（!!#e06666  *注意：vocab_size，vocab_list，vocab_file只能设置其中某一个* !!）|  
|**vocab_list**|否|list|当是类别特征且类别数不多，可以通过vocab_list枚举出每个分类的取值。!!#e06666  *注意：vocab_size，vocab_list，vocab_file只能设置其中某一个* !!）|  
|**vocab_file**|否|string|当是类别特征时且类别取值很多，可以把所有取值放在一个文件中，每行一个取值，vocab_file设置为对应文件路径。!!#e06666  *注意：vocab_size，vocab_list，vocab_file只能设置其中某一个* !!）|  
|**num_oov_buckets**|否|int|对于oov id的hash分桶数，如果不配置则把oov都映射到统一的一个id上|  
|**embedding_dim**|否|int|如果需要对特征做embedding，使用embedding_dim指定embedding的维度大小|  
|**embedding_combiner**|否|string|对于序列类别特征，可以对序列中每个元素对应的embedding做pooling。embedding_combiner指定pooling方式，目前支持"max"， "min"，"sum"，"mean"几种|  
|**embedding_l1_reg**|否|float|embedding的L1正则化系数|  
|**embedding_l2_reg**|否|float|embedding的L2正则化系数|  
|**embedding_name**|否|string|对于要做embedding的特征列，可以为embedding取一个名字，所有embedding_name相同的特征列将共享同样的embedding向量空间|  
|**one_hot**|否|bool|如果是类别特征，是否要对类别特征做one_hot编码，编码维度大小通过vocab_size指定|  
|**bucket_boundaries**|否|list|如果是连续特征，且需要做分桶处理，则通过bucket_boundaries这只分桶的边界值。分桶区间为左闭右开。|  
|**is_label**|否|bool|指定该列是否是label列|  
|**is_sample_weight**|否|bool|如果有样本权重，需要单独一列来表示权重，对于样本权重列设置此参数为true。!!#e06666 注意：最多只能有一列样本权重列!!|  
|**hash_type**|否|string|有效取值为"**hash**"和"**mod**"，主要影响embedding特征和one hot特征，"hash"表示先对原始特征值做hash，hash空间大小为vocab_size；"mod"表示先对原始特征值取模，模大小为vocab_size。对于用户已经定义好类别编码且是连续整数的可以用"mod"。|  
|**zscore**|否|bool|该列是否要做zscore变换，!!#cc0000 如果为true，必须要同时指定std和mean值!!|  
|**std**|否|float|该列标准差值|  
|**mean**|否|float|该列平均值|  
|**weight_col**|否|string|为**序列特征列**指定权重列，权重是指序列中每一个元素的权重，所以!!#ff0000 权重列也必须是一个序列，且元素个数与序列特征列是一样!!的。|  
|**exclude**|否|bool|true表示该列不会进入模型作为特征，默认为false。在特殊情况下有用，例如使用gauc时需要数据中有uid标识，但是uid本身又不是模型的特征之一。|  
|**self_weighted**|否|bool|是否使用自身原始值作为embedding权重（即使用自身原始值乘以embedding之后的值）。!!#cc0000 主要使用场景：1.对连续值也做embedding，此时配置vocab_size=1, embedding_dim=<维度>, self_weighted=true。2.对连续值分桶之后做embedding，并用原始值做权重，此时配置bucket_boundaries=<桶边界>, embedding_dim=<维度>, self_weighted=true!!|  
  
# 三.feature column描述格式(fc_desc)  
feature column描述是一个json dict，包含如下一些字段：  
  
| 字段名 | 是否必填 | 取值类型 | 说明 |  
| ------ | ------ | ------ | ------ |  
| **op** | 是 | string | 创建feature column的操作类型，支持的操作类型见后面表 |  
| **in** | 是 | string/list | feature column操作的输入特征名，输入可以来自原始输入，也可以来自其他feature column操作的输出。 如果有多个输入，则是输入特征名的列表。|  
|**out**|否|string/list|feature column操作的输出特征名，如果有多个输出，则是输出名列表。不填时会自动生成特征名。如果输出的特征要用于后面与其他特征做交叉，则这里最好对输出特征进行命名，便于后续交叉引用|  
|**args**|否|dict|feature column操作对应的参数，不同的操作对应的参数可能不一样。见后表说明|  
  
支持的feature column操作及参数如下表：  
  
| op | 对应tf feature column接口| args | 说明 |  
| ------ | ------ | ------ | ------ |  
| **bucket** | tf.feature_column.bucketized_column | 必填参数：<br>- boundaries:分桶边界<br> | 对特征值做分桶处理，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/bucketized_column |  
| **hash_bucket** | tf.feature_column.categorical_column_with_hash_bucket | 必填参数：<br>- hash_bucket_size：hash之后分桶数 | 对特征值做hash之后再分桶，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket |  
|**identity**|tf.feature_column.categorical_column_with_identity|必填参数：<br>- num_buckets: 映射最大值|特征值映射到自身，如果超出num_buckets，且没有设置defaule_value参数，则报错，否则映射到default_value。详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/categorical_column_with_identity|  
|**vocab_list**|tf.feature_column.categorical_column_with_vocabulary_list|必填参数：<br>- vocabulary_list：可取值列表|定义离散特征的取值列表，将离散特征映射为其对应的列表下标，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list|  
|**vocab_file**|tf.feature_column.categorical_column_with_vocabulary_file|必填参数：<br>- vocabulary_file: 包含可取值列表的文件路径|与vocab_list一样，只是可取值保存在一个文件中，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file|  
|**cross**|tf.feature_column.crossed_column|必填参数：<br>- hash_bucket_size: 交叉之后特征hash桶数|特征交叉，将交叉后的离散值映射为其对应的哈希值，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/crossed_column|  
|**embedding**|tf.feature_column.embedding_column|必填参数：<br>- dimension: embedding维度大小|对类别特征值做embedding，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/embedding_column|  
|**share_embedding**|tf.feature_column.shared_embeddings|必填参数：<br>- dimension: embedding维度大小|对类别特征值做embedding，多个输入特征共享同样的embedding向量空间，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/shared_embeddings|  
|**numeric**|tf.feature_column.numeric_column|必填参数：无|表示输入特征值是连续数值类型，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/numeric_column|  
|**indicator**|tf.feature_column.indicator_column|必填参数：无|将输入特征表示为multi-hot形式，详细说明见：https://tensorflow.google.cn/api_docs/python/tf/feature_column/indicator_column|  
  
# 四.完整示例  
1.不带feature column:  
```   
{  
  "inputs": [  
    {"name": "shared_songid_list_30d", "dtype": "int64", "val_sep": ",", "vocab_size": 300000, "embedding_name": "item_id"},  
    {"name": "down_songid_list_30d", "dtype": "int64", "val_sep": ",", "vocab_size": 300000, "embedding_name": "item_id"},  
    {"name": "favor_songid_list_30d", "dtype": "int64", "val_sep": ",", "vocab_size": 300000, "embedding_name": "item_id"},  
    {"name": "search_oper_cnt", "dtype": "float32"},  
    {"name": "favorite_oper_cnt", "dtype": "float32"},  
    {"name": "success_dl_cnt", "dtype": "float32"},  
    {"name": "share_oper_cnt", "dtype": "float32"},  
    {"name": "mv_oper_cnt", "dtype": "float32"},  
    {"name": "play_cnt", "dtype": "float32"},  
    {"name": "age", "dtype": "int32", "bucket_boundaries": [0, 10, 20, 35, 50, 65, 80, 100], "one_hot": true},  
    {"name": "gender", "dtype": "int32", "vocab_size": 3},  
    {"name": "recent_country_id", "dtype": "int32", "vocab_size": 150, "embedding_dim": 8},  
    {"name": "recent_province_id", "dtype": "int32", "vocab_size": 50, "embedding_dim": 8},  
    {"name": "recent_city_id", "dtype": "int32", "vocab_size": 400, "embedding_dim": 8},  
    {"name": "degree", "dtype": "int32", "vocab_size": 9},  
    {"name": "income_level", "dtype": "int32", "vocab_list": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "embedding_dim": 4},  
    {"name": "city_level", "dtype": "int32", "vocab_size": 7},  
    {"name": "songid_label", "dtype": "float32", "is_label": true}  
  ]  
}  
```  
  
2.带feature column：  
  
```   
{  
  "inputs": [  
    {"name": "shared_songid_list_30d", "dtype": "int64", "val_sep": ",", "vocab_size": 300000, "embedding_name": "item_id"},  
    {"name": "down_songid_list_30d", "dtype": "int64", "val_sep": ",", "vocab_size": 300000, "embedding_name": "item_id"},  
    {"name": "favor_songid_list_30d", "dtype": "int64", "val_sep": ",", "vocab_size": 300000, "embedding_name": "item_id"},  
    {"name": "search_oper_cnt", "dtype": "float32"},  
    {"name": "favorite_oper_cnt", "dtype": "float32"},  
    {"name": "success_dl_cnt", "dtype": "float32"},  
    {"name": "share_oper_cnt", "dtype": "float32"},  
    {"name": "mv_oper_cnt", "dtype": "float32"},  
    {"name": "play_cnt", "dtype": "float32"},  
    {"name": "age", "dtype": "int32"},  
    {"name": "gender", "dtype": "int32", "vocab_size": 3},  
    {"name": "recent_country_id", "dtype": "int32", "vocab_size": 150, "embedding_dim": 8},  
    {"name": "recent_province_id", "dtype": "int32", "vocab_size": 50, "embedding_dim": 8},  
    {"name": "recent_city_id", "dtype": "int32", "vocab_size": 400, "embedding_dim": 8},  
    {"name": "degree", "dtype": "int32", "vocab_size": 9},  
    {"name": "income_level", "dtype": "int32", "vocab_size": 10},  
    {"name": "city_level", "dtype": "int32", "vocab_size": 7},  
    {"name": "songid_label", "dtype": "float32", "is_label": true}  
  ],  
  "feature_columns": {  
    "defs": [  
      {  
        "op": "hash_bucket",  
        "in": "shared_songid_list_30d",  
        "out": "shared_songid_list_30d_hash",  
        "args": {  
          "hash_bucket_size": 30000  
        }  
      },  
      {  
        "op": "hash_bucket",  
        "in": "down_songid_list_30d",  
        "out": "down_songid_list_30d_hash",  
        "args": {  
          "hash_bucket_size": 30000  
        }  
      },  
      {  
        "op": "numeric",  
        "in": "search_oper_cnt"  
      },  
      {  
        "op": "numeric",  
        "in": "favorite_oper_cnt"  
      },  
      {  
        "op": "bucket",  
        "in": "age",  
        "out": "age_cate",  
        "args": {  
          "boundaries": [0, 10, 20, 35, 50, 65, 80, 100]  
        }  
      },  
      {  
        "op": "identity",  
        "in": "gender",  
        "out": "gender_cate",  
        "args": {  
          "num_buckets": 3,  
          "default_value": 0  
        }  
      },  
      {  
        "op": "vocab_list",  
        "in": "income_level",  
        "out": "income_level_cate",  
        "args": {  
          "vocabulary_list": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  
          "default_value": 0  
        }  
      },  
      {  
        "op": "cross",  
        "in": ["shared_songid_list_30d", "gender"],  
        "out": "shared_songid_list_30d_cross_gender",  
        "args": {  
          "hash_bucket_size": 100000  
        }  
      },  
      {  
        "op": "cross",  
        "in": ["age_cate", "gender_cate"],  
        "out": "age_cross_gender",  
        "args": {  
          "hash_bucket_size": 20  
        }  
      },  
      {  
        "op": "cross",  
        "in": ["age_cate", "gender_cate", "income_level_cate"],  
        "out": "age_cross_gender_cross_income",  
        "args": {  
          "hash_bucket_size": 100  
        }  
      }  
    ]  
  }  
}  
```  
  
  
  
  
  
